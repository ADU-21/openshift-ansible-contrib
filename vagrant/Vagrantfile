# -*- mode: ruby -*-
# vi: set ft=ruby :

require 'socket'

hostname = Socket.gethostname
localmachineip = IPSocket.getaddress(Socket.gethostname)
puts %Q{ This machine has the IP '#{localmachineip} and host name '#{hostname}'}

# Vagrantfile API/syntax version. Don't touch unless you know what you're doing!
VAGRANTFILE_API_VERSION = '2'

deployment_type = ENV['DEPLOYMENT_TYPE'] || 'origin'
origin_os = ENV['ORIGIN_OS'] || 'centos'

if deployment_type == 'openshift-enterprise'
  REQUIRED_PLUGINS = %w(vagrant-registration vagrant-hostmanager landrush)
else
  REQUIRED_PLUGINS = %w(vagrant-hostmanager landrush)
end

errors = []

def message(name)
  "#{name} plugin is not installed, run `vagrant plugin install #{name}` to install it."
end
# Validate and collect error message if plugin is not installed
REQUIRED_PLUGINS.each { |plugin| errors << message(plugin) unless Vagrant.has_plugin?(plugin) }
unless errors.empty?
  msg = errors.size > 1 ? "Errors: \n* #{errors.join("\n* ")}" : "Error: #{errors.first}"
  fail Vagrant::Errors::VagrantError.new, msg
end

if deployment_type == 'openshift-enterprise'
  box_name = 'rhel/7.2'
elsif origin_os == 'centos'
  box_name = 'centos/7'
else
  box_name = 'fedora/24-cloud-base'
end

NETWORK_BASE = '192.168.50'
INTEGRATION_START_SEGMENT = 20

Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|

#  if Vagrant.has_plugin?("vagrant-cachier")
#    config.cache.scope = :machine
#  end

  config.hostmanager.enabled = true
  config.hostmanager.manage_host = true
  config.hostmanager.ignore_private_ip = false
#  config.hostmanager.include_offline = true

  config.landrush.enabled = true
  config.landrush.tld = 'example.com'
  config.landrush.guest_redirect_dns = false

  if deployment_type == 'openshift-enterprise'
    # vagrant-registration
    if ENV.has_key?('SUB_USERNAME') && ENV.has_key?('SUB_PASSWORD')
      config.registration.username = ENV['SUB_USERNAME']
      config.registration.password = ENV['SUB_PASSWORD']
    end

    # Proxy Information from environment
    config.registration.proxy = PROXY = (ENV['PROXY'] || '')
    config.registration.proxyUser = PROXY_USER = (ENV['PROXY_USER'] || '')
    config.registration.proxyPassword = PROXY_PASSWORD = (ENV['PROXY_PASSWORD'] || '')
    config.registration.auto_attach = true
  end

  config.vm.provider "virtualbox" do |v, override|
    #v.customize ["setextradata", :id, "VBoxInternal2/SharedFoldersEnableSymlinksCreate//vagrant","1"]
    v.memory = 1024
    v.cpus = 1
    override.vm.box = box_name
    override.vm.synced_folder '.', '/vagrant'
    provider_name = 'virtualbox'
  end

  config.vm.provider "libvirt" do |libvirt, override|
    libvirt.cpus = 1
    libvirt.memory = 1024
    libvirt.driver = 'kvm'
    override.vm.box = box_name
    override.vm.synced_folder ".", "/vagrant", type: "nfs"
    provider_name = 'libvirt'
  end

  config.vm.synced_folder '.', '/home/vagrant/sync', disabled: true

  config.vm.define "master1" do |master1|
    master1.vm.network :private_network, ip: "#{NETWORK_BASE}.#{INTEGRATION_START_SEGMENT}"
    master1.vm.hostname = "master1.example.com"
  end

  config.vm.define "node1" do |node1|
    node1.vm.network :private_network, ip: "#{NETWORK_BASE}.#{INTEGRATION_START_SEGMENT + 1}"
    node1.vm.hostname = "node1.example.com"
  end

  config.vm.define "node2" do |node2|
    node2.vm.network :private_network, ip: "#{NETWORK_BASE}.#{INTEGRATION_START_SEGMENT + 2}"
    node2.vm.hostname = "node2.example.com"
  end

  config.vm.define "admin1" do |admin1|
    admin1.vm.network :private_network, ip: "#{NETWORK_BASE}.#{INTEGRATION_START_SEGMENT + 3}"
    admin1.vm.hostname = "admin1.example.com"

    if deployment_type == 'openshift-enterprise'
      config.vm.provision "shell", inline: <<-SHELL
        sudo yum -y install atomic-openshift-utils
      SHELL
    else
      config.vm.provision "shell", inline: <<-SHELL
        rm -rf openshift-ansible || true
        git clone https://github.com/openshift/openshift-ansible
        sudo yum -y install https://dl.fedoraproject.org/pub/epel/7/x86_64/e/epel-release-7-8.noarch.rpm
        sudo sed -i -e "s/^enabled=1/enabled=0/" /etc/yum.repos.d/epel.repo
        sudo yum -y --enablerepo=epel install ansible pyOpenSSL
      SHELL
    end

    config.vm.provision "shell", inline: <<-SHELL
      pushd /home/vagrant/.ssh
      for key in `find /vagrant/.vagrant/machines -name private_key`; do
        machine=$(echo $key | cut -d '/' -f 5)
        ln -s $key $machine.key
      done
      popd
    SHELL

    admin1.vm.provision :ansible_local do |ansible|
      ansible.verbose        = true
      ansible.install        = false
      ansible.limit          = "OSEv3:localhost"
      if deployment_type == 'openshift-enterprise'
        ansible.playbook = "/usr/share/ansible/openshift-ansible/playbooks/byo/config.yml"
      else
        ansible.playbook = "/home/vagrant/openshift-ansible/playbooks/byo/config.yml"
      end
      ansible.groups = {
        OSEv3: ["master1", "node1", "node2"],
        'OSEv3:children': ["masters", "nodes", "etcd", "nfs"],
        'OSEv3:vars': {
          ansible_become: true,
          ansible_ssh_user: 'vagrant',
          deployment_type: "#{deployment_type}",
          openshift_master_identity_providers: [
            {
              name: 'htpasswd_auth',
              login: 'true',
              challenge: 'true',
              kind: 'HTPasswdPasswordIdentityProvider',
              filename: '/etc/origin/master/htpasswd'
            }
          ],
          openshift_master_htpasswd_users: {
            # admin: admin123
            admin: '$2y$11$jJioXC3WgyRq.FVy1vqtfuywDwEZp18d9Kkqb4MgFVzlgCGQNwy36',
          },
          openshift_master_default_subdomain: 'apps.example.com',
          osm_default_node_selector: 'region=infra',
          openshift_hosted_registry_selector: 'region=infra',
          openshift_hosted_registry_replicas: 1,
          openshift_hosted_registry_storage_kind: 'nfs',
          openshift_hosted_registry_storage_access_modes: ['ReadWriteMany'],
          openshift_hosted_registry_storage_host: 'admin1.example.com',
          openshift_hosted_registry_storage_nfs_directory: '/srv/nfs',
          openshift_hosted_registry_storage_volume_name: 'registry',
          openshift_hosted_registry_storage_volume_size: '2Gi',

        },
        etcd: ["master1"],
        masters: ["master1"],
        nodes: ["master1", "node1", "node2"],
      }
      ansible.host_vars = {
        master1:  {
          openshift_ip: '192.168.50.20',
          openshift_node_labels: {
            region: 'infra',
            zone: 'default'
          },
          openshift_schedulable: true,
          ansible_host: '192.168.50.20',
          ansible_ssh_private_key_file: "/home/vagrant/.ssh/master1.key"
        },
        node1: {
          openshift_ip: '192.168.50.21',
          openshift_node_labels: {
            region: 'primary',
            zone: 'east'
          },
          openshift_schedulable: true,
          ansible_host: '192.168.50.21',
          ansible_ssh_private_key_file: "/home/vagrant/.ssh/node1.key"
        },
        node2: {
          openshift_ip: '192.168.50.22',
          openshift_node_labels: {
            region: 'primary',
            zone: 'west'
          },
          openshift_schedulable: true,
          ansible_host: '192.168.50.22',
          ansible_ssh_private_key_file: "/home/vagrant/.ssh/node2.key"
        },
        admin1: {
          ansible_connection: 'local'
        }
      }
    end
  end


  if deployment_type == 'openshift-enterprise'
    subscription_steps = <<-SUB_STEPS
      echo "Setting up Red Hat subscriptions"
      POOL_ID=$(sudo subscription-manager list --available | sed -n '/Employee SKU/,/System Type/p' | grep "Pool ID" | tail -1 | cut -d':' -f2 | xargs)
      echo -e $GREEN"Trying PoolID: $POOL_ID"$WHITE
      sudo subscription-manager attach --pool=$POOL_ID
      sudo subscription-manager repos --disable="*"
      sudo subscription-manager repos --enable="rhel-7-server-rpms" --enable="rhel-7-server-extras-rpms" --enable="rhel-7-server-ose-3.3-rpms"
    SUB_STEPS
  else
    subscription_steps = ''
  end

  config.vm.provision "shell", inline: <<-SHELL
    #{subscription_steps}

    sudo yum -y install git bash-completion

    # Vagrant's "change host name" capability for Fedora
    # maps hostname to loopback, conflicting with hostmanager.
    # We must repair /etc/hosts
    sudo sed -i '/127\.0\.0\.1\s.*example.*/d' /etc/hosts
  SHELL

end
